{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabWork2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9I_cx5EvPc",
        "colab_type": "text"
      },
      "source": [
        "Инициализация библиотек и модулей необходимых для работы, а также аутентификация в системе Google для получения доступа к диску"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtPD_MZST-cs",
        "colab_type": "code",
        "outputId": "2948edf1-e3a1-4cfd-b85d-8c4d0768a70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTd3sfl2Eh-o",
        "colab_type": "text"
      },
      "source": [
        "Загрузка файла с текстом из Google-диск"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnMa288-tEC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1G1WWjsQLZwbwwe9jzyAshtE36dggjnOs\"})\n",
        "downloaded.GetContentFile('Work_text(1).txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSB-oxSMFInt",
        "colab_type": "text"
      },
      "source": [
        "Загрузка текста ASCII для книги в память и преобразование всех символов в нижний регистр, чтобы уменьшить словарный запас, который должна выучить сеть."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhnI92R7qX3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Work_text(1).txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEl-fs_fGj1M",
        "colab_type": "text"
      },
      "source": [
        "Создание набора всех отдельных символов в книге, а затем создание карты каждого символа с уникальным целым числом. Так как мы не можем моделировать символы напрямую."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3-ASBhXtjlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VUPCfocHD7B",
        "colab_type": "text"
      },
      "source": [
        "Суммируем набор данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5sGBwqwtqfm",
        "colab_type": "code",
        "outputId": "04efe33a-03c0-42a1-e524-4c36e3178710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  31202\n",
            "Total Vocab:  67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VUWr-XdHRRL",
        "colab_type": "text"
      },
      "source": [
        "Разделим текст книги на подпоследовательности с фиксированной длиной в 100 символов произвольной длины.Каждый обучающий шаблон сети будет состоять из 100 временных шагов одного символа (X), за которыми следует один символьный вывод (y). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1Rnl6DCuIO2",
        "colab_type": "code",
        "outputId": "9085c658-e7a2-472e-eaf2-8daf925d1def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  31102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14-RGbGEIPVH",
        "colab_type": "text"
      },
      "source": [
        "Нужно преобразовать тренировочные данные так, чтобы они подходили для использования с Keras. Сначала преобразуем список входных последовательностей. Далее изменить масштаб целых чисел в диапазоне от 0 до 1, чтобы облегчить изучение шаблонов сетью LSTM, которая по умолчанию использует функцию активации сигмоида. Далее произведём one-hot encoding для выходных шаблонов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En02uKLtuTX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RyGXEYXKtU6",
        "colab_type": "text"
      },
      "source": [
        "Инициализируем модель и определяем её архитектуру, а также визуализируем средствами keras для наглядности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFRhB9GMuW0O",
        "colab_type": "code",
        "outputId": "9fe327d0-5427-4aff-847c-8466e0470aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlrGa1ALNPc",
        "colab_type": "text"
      },
      "source": [
        "Будем делать бэкап весов по каждой эпоху обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4knLBn6gueXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHw6nNwLnnG",
        "colab_type": "text"
      },
      "source": [
        "Тренируем модель, на вход количество эпох - 50 , размер батча у нас это 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvPzW1RyuiaW",
        "colab_type": "code",
        "outputId": "fe52abde-b25c-453c-8d97-cfb7dd24806e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "31102/31102 [==============================] - 208s 7ms/step - loss: 3.0556\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.05563, saving model to weights-improvement-01-3.0556.hdf5\n",
            "Epoch 2/50\n",
            "31102/31102 [==============================] - 208s 7ms/step - loss: 2.9918\n",
            "\n",
            "Epoch 00002: loss improved from 3.05563 to 2.99178, saving model to weights-improvement-02-2.9918.hdf5\n",
            "Epoch 3/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.9653\n",
            "\n",
            "Epoch 00003: loss improved from 2.99178 to 2.96534, saving model to weights-improvement-03-2.9653.hdf5\n",
            "Epoch 4/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.8900\n",
            "\n",
            "Epoch 00004: loss improved from 2.96534 to 2.88998, saving model to weights-improvement-04-2.8900.hdf5\n",
            "Epoch 5/50\n",
            "31102/31102 [==============================] - 209s 7ms/step - loss: 2.8270\n",
            "\n",
            "Epoch 00005: loss improved from 2.88998 to 2.82704, saving model to weights-improvement-05-2.8270.hdf5\n",
            "Epoch 6/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.7783\n",
            "\n",
            "Epoch 00006: loss improved from 2.82704 to 2.77830, saving model to weights-improvement-06-2.7783.hdf5\n",
            "Epoch 7/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.7327\n",
            "\n",
            "Epoch 00007: loss improved from 2.77830 to 2.73266, saving model to weights-improvement-07-2.7327.hdf5\n",
            "Epoch 8/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.6961\n",
            "\n",
            "Epoch 00008: loss improved from 2.73266 to 2.69608, saving model to weights-improvement-08-2.6961.hdf5\n",
            "Epoch 9/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.6636\n",
            "\n",
            "Epoch 00009: loss improved from 2.69608 to 2.66358, saving model to weights-improvement-09-2.6636.hdf5\n",
            "Epoch 10/50\n",
            "31102/31102 [==============================] - 208s 7ms/step - loss: 2.6358\n",
            "\n",
            "Epoch 00010: loss improved from 2.66358 to 2.63582, saving model to weights-improvement-10-2.6358.hdf5\n",
            "Epoch 11/50\n",
            "31102/31102 [==============================] - 209s 7ms/step - loss: 2.6255\n",
            "\n",
            "Epoch 00011: loss improved from 2.63582 to 2.62552, saving model to weights-improvement-11-2.6255.hdf5\n",
            "Epoch 12/50\n",
            "31102/31102 [==============================] - 209s 7ms/step - loss: 2.6315\n",
            "\n",
            "Epoch 00012: loss did not improve from 2.62552\n",
            "Epoch 13/50\n",
            "31102/31102 [==============================] - 205s 7ms/step - loss: 2.6368\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.62552\n",
            "Epoch 14/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.5761\n",
            "\n",
            "Epoch 00014: loss improved from 2.62552 to 2.57608, saving model to weights-improvement-14-2.5761.hdf5\n",
            "Epoch 15/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.5619\n",
            "\n",
            "Epoch 00015: loss improved from 2.57608 to 2.56189, saving model to weights-improvement-15-2.5619.hdf5\n",
            "Epoch 16/50\n",
            "31102/31102 [==============================] - 205s 7ms/step - loss: 2.5509\n",
            "\n",
            "Epoch 00016: loss improved from 2.56189 to 2.55091, saving model to weights-improvement-16-2.5509.hdf5\n",
            "Epoch 17/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.5451\n",
            "\n",
            "Epoch 00017: loss improved from 2.55091 to 2.54511, saving model to weights-improvement-17-2.5451.hdf5\n",
            "Epoch 18/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.5299\n",
            "\n",
            "Epoch 00018: loss improved from 2.54511 to 2.52994, saving model to weights-improvement-18-2.5299.hdf5\n",
            "Epoch 19/50\n",
            "31102/31102 [==============================] - 203s 7ms/step - loss: 2.5224\n",
            "\n",
            "Epoch 00019: loss improved from 2.52994 to 2.52238, saving model to weights-improvement-19-2.5224.hdf5\n",
            "Epoch 20/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.5156\n",
            "\n",
            "Epoch 00020: loss improved from 2.52238 to 2.51555, saving model to weights-improvement-20-2.5156.hdf5\n",
            "Epoch 21/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.5032\n",
            "\n",
            "Epoch 00021: loss improved from 2.51555 to 2.50319, saving model to weights-improvement-21-2.5032.hdf5\n",
            "Epoch 22/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.4992\n",
            "\n",
            "Epoch 00022: loss improved from 2.50319 to 2.49924, saving model to weights-improvement-22-2.4992.hdf5\n",
            "Epoch 23/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.4847\n",
            "\n",
            "Epoch 00023: loss improved from 2.49924 to 2.48474, saving model to weights-improvement-23-2.4847.hdf5\n",
            "Epoch 24/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.4779\n",
            "\n",
            "Epoch 00024: loss improved from 2.48474 to 2.47787, saving model to weights-improvement-24-2.4779.hdf5\n",
            "Epoch 25/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.4626\n",
            "\n",
            "Epoch 00025: loss improved from 2.47787 to 2.46262, saving model to weights-improvement-25-2.4626.hdf5\n",
            "Epoch 26/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.4556\n",
            "\n",
            "Epoch 00026: loss improved from 2.46262 to 2.45560, saving model to weights-improvement-26-2.4556.hdf5\n",
            "Epoch 27/50\n",
            "31102/31102 [==============================] - 204s 7ms/step - loss: 2.4486\n",
            "\n",
            "Epoch 00027: loss improved from 2.45560 to 2.44861, saving model to weights-improvement-27-2.4486.hdf5\n",
            "Epoch 28/50\n",
            "31102/31102 [==============================] - 203s 7ms/step - loss: 2.4312\n",
            "\n",
            "Epoch 00028: loss improved from 2.44861 to 2.43115, saving model to weights-improvement-28-2.4312.hdf5\n",
            "Epoch 29/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.4219\n",
            "\n",
            "Epoch 00029: loss improved from 2.43115 to 2.42188, saving model to weights-improvement-29-2.4219.hdf5\n",
            "Epoch 30/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.4110\n",
            "\n",
            "Epoch 00030: loss improved from 2.42188 to 2.41100, saving model to weights-improvement-30-2.4110.hdf5\n",
            "Epoch 31/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.3933\n",
            "\n",
            "Epoch 00031: loss improved from 2.41100 to 2.39326, saving model to weights-improvement-31-2.3933.hdf5\n",
            "Epoch 32/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.3849\n",
            "\n",
            "Epoch 00032: loss improved from 2.39326 to 2.38490, saving model to weights-improvement-32-2.3849.hdf5\n",
            "Epoch 33/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.3691\n",
            "\n",
            "Epoch 00033: loss improved from 2.38490 to 2.36905, saving model to weights-improvement-33-2.3691.hdf5\n",
            "Epoch 34/50\n",
            "31102/31102 [==============================] - 205s 7ms/step - loss: 2.3537\n",
            "\n",
            "Epoch 00034: loss improved from 2.36905 to 2.35365, saving model to weights-improvement-34-2.3537.hdf5\n",
            "Epoch 35/50\n",
            "31102/31102 [==============================] - 205s 7ms/step - loss: 2.3373\n",
            "\n",
            "Epoch 00035: loss improved from 2.35365 to 2.33731, saving model to weights-improvement-35-2.3373.hdf5\n",
            "Epoch 36/50\n",
            "31102/31102 [==============================] - 206s 7ms/step - loss: 2.3177\n",
            "\n",
            "Epoch 00036: loss improved from 2.33731 to 2.31768, saving model to weights-improvement-36-2.3177.hdf5\n",
            "Epoch 37/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.3044\n",
            "\n",
            "Epoch 00037: loss improved from 2.31768 to 2.30443, saving model to weights-improvement-37-2.3044.hdf5\n",
            "Epoch 38/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.3164\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.30443\n",
            "Epoch 39/50\n",
            "31102/31102 [==============================] - 214s 7ms/step - loss: 2.2692\n",
            "\n",
            "Epoch 00039: loss improved from 2.30443 to 2.26916, saving model to weights-improvement-39-2.2692.hdf5\n",
            "Epoch 40/50\n",
            "31102/31102 [==============================] - 214s 7ms/step - loss: 2.2510\n",
            "\n",
            "Epoch 00040: loss improved from 2.26916 to 2.25100, saving model to weights-improvement-40-2.2510.hdf5\n",
            "Epoch 41/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.2357\n",
            "\n",
            "Epoch 00041: loss improved from 2.25100 to 2.23574, saving model to weights-improvement-41-2.2357.hdf5\n",
            "Epoch 42/50\n",
            "31102/31102 [==============================] - 208s 7ms/step - loss: 2.2211\n",
            "\n",
            "Epoch 00042: loss improved from 2.23574 to 2.22106, saving model to weights-improvement-42-2.2211.hdf5\n",
            "Epoch 43/50\n",
            "31102/31102 [==============================] - 214s 7ms/step - loss: 2.2030\n",
            "\n",
            "Epoch 00043: loss improved from 2.22106 to 2.20298, saving model to weights-improvement-43-2.2030.hdf5\n",
            "Epoch 44/50\n",
            "31102/31102 [==============================] - 218s 7ms/step - loss: 2.1881\n",
            "\n",
            "Epoch 00044: loss improved from 2.20298 to 2.18812, saving model to weights-improvement-44-2.1881.hdf5\n",
            "Epoch 45/50\n",
            "31102/31102 [==============================] - 216s 7ms/step - loss: 2.1675\n",
            "\n",
            "Epoch 00045: loss improved from 2.18812 to 2.16750, saving model to weights-improvement-45-2.1675.hdf5\n",
            "Epoch 46/50\n",
            "31102/31102 [==============================] - 216s 7ms/step - loss: 2.1511\n",
            "\n",
            "Epoch 00046: loss improved from 2.16750 to 2.15107, saving model to weights-improvement-46-2.1511.hdf5\n",
            "Epoch 47/50\n",
            "31102/31102 [==============================] - 210s 7ms/step - loss: 2.1339\n",
            "\n",
            "Epoch 00047: loss improved from 2.15107 to 2.13394, saving model to weights-improvement-47-2.1339.hdf5\n",
            "Epoch 48/50\n",
            "31102/31102 [==============================] - 208s 7ms/step - loss: 2.1108\n",
            "\n",
            "Epoch 00048: loss improved from 2.13394 to 2.11083, saving model to weights-improvement-48-2.1108.hdf5\n",
            "Epoch 49/50\n",
            "31102/31102 [==============================] - 207s 7ms/step - loss: 2.0910\n",
            "\n",
            "Epoch 00049: loss improved from 2.11083 to 2.09103, saving model to weights-improvement-49-2.0910.hdf5\n",
            "Epoch 50/50\n",
            "31102/31102 [==============================] - 209s 7ms/step - loss: 2.0707\n",
            "\n",
            "Epoch 00050: loss improved from 2.09103 to 2.07073, saving model to weights-improvement-50-2.0707.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb01eebe668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7MCEtZGMRJV",
        "colab_type": "text"
      },
      "source": [
        "Подгружаем в модель веса с наилучшим результатом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr4w19AJaVlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"weights-improvement-50-2.0707.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bya0pwMgyEZX",
        "colab_type": "text"
      },
      "source": [
        "Преобразование обратно в символы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKd1U0yCc2-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RniIq4ujNRp2",
        "colab_type": "text"
      },
      "source": [
        "Пробуем делать прогнозы на обученной модели. Сначала начнём с последовательности начальных чисел в качестве входных данных, сгенерируем следующий символ, затем обновим последовательность начальных чисел, чтобы добавить сгенерированный символ в конец, и обрезать первый символ. Этот процесс повторяется до тех пор, пока мы хотим предсказать новые символы (например, последовательность длиной 1000 символов).\n",
        "\n",
        "Мы можем выбрать случайный шаблон ввода в качестве нашей начальной последовательности, а затем распечатать сгенерированные символы по мере их создания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6fqnayac5KA",
        "colab_type": "code",
        "outputId": "97120b6a-bf5c-40fa-c959-015a5f98fb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\t#sys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"                                                   w. ben hunt\n",
            "\n",
            "\n",
            "milwaukee public museum\n",
            "\n",
            "        pop \"\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VuZzrp0N35y",
        "colab_type": "text"
      },
      "source": [
        "Аналогичный процесс произведём для двухслойной LSTM, однослойной GRU, и SimpleRNN, полноценные результаты и выводы по экспериментам отражены в прилагающемся отчёте."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWFHvQ_SeP76",
        "colab_type": "code",
        "outputId": "161b26af-7522-438d-eff5-9ed59e7b74a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Two layers LSTM Network to Generate Text for \"KACHINA DOLLS\"\n",
        "import numpy\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"Work_text(1).txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "#model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  31202\n",
            "Total Vocab:  67\n",
            "Total Patterns:  31102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZsFXTZDWe-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"weights-improvement-50-0.8987-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW4fbGnLXFdg",
        "colab_type": "code",
        "outputId": "02e1c5e7-e448-4b17-a2f7-561a6749a474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\t#sys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" n it was\n",
            "discovered that tourists prized them. and like everything else, someone\n",
            "saw a chance to ear \"\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3UUwHcvZBIv",
        "colab_type": "code",
        "outputId": "c2825382-bc91-4507-dbdf-1da808b79817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# One lauer GRU Network to Generate Text \"KACHINA DOLLS\"\n",
        "import numpy\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"Work_text(1).txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the GRU model\n",
        "model = Sequential()\n",
        "model.add(GRU(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, nb_epoch=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  31202\n",
            "Total Vocab:  67\n",
            "Total Patterns:  31102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "31102/31102 [==============================] - 147s 5ms/step - loss: 3.0152\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.01524, saving model to weights-improvement-01-3.0152-bigger.hdf5\n",
            "Epoch 2/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 2.8332\n",
            "\n",
            "Epoch 00002: loss improved from 3.01524 to 2.83319, saving model to weights-improvement-02-2.8332-bigger.hdf5\n",
            "Epoch 3/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.7467\n",
            "\n",
            "Epoch 00003: loss improved from 2.83319 to 2.74666, saving model to weights-improvement-03-2.7467-bigger.hdf5\n",
            "Epoch 4/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.6841\n",
            "\n",
            "Epoch 00004: loss improved from 2.74666 to 2.68414, saving model to weights-improvement-04-2.6841-bigger.hdf5\n",
            "Epoch 5/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.6263\n",
            "\n",
            "Epoch 00005: loss improved from 2.68414 to 2.62630, saving model to weights-improvement-05-2.6263-bigger.hdf5\n",
            "Epoch 6/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.5840\n",
            "\n",
            "Epoch 00006: loss improved from 2.62630 to 2.58400, saving model to weights-improvement-06-2.5840-bigger.hdf5\n",
            "Epoch 7/50\n",
            "31102/31102 [==============================] - 152s 5ms/step - loss: 2.5515\n",
            "\n",
            "Epoch 00007: loss improved from 2.58400 to 2.55148, saving model to weights-improvement-07-2.5515-bigger.hdf5\n",
            "Epoch 8/50\n",
            "31102/31102 [==============================] - 151s 5ms/step - loss: 2.5222\n",
            "\n",
            "Epoch 00008: loss improved from 2.55148 to 2.52224, saving model to weights-improvement-08-2.5222-bigger.hdf5\n",
            "Epoch 9/50\n",
            "31102/31102 [==============================] - 153s 5ms/step - loss: 2.4963\n",
            "\n",
            "Epoch 00009: loss improved from 2.52224 to 2.49634, saving model to weights-improvement-09-2.4963-bigger.hdf5\n",
            "Epoch 10/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 2.4721\n",
            "\n",
            "Epoch 00010: loss improved from 2.49634 to 2.47210, saving model to weights-improvement-10-2.4721-bigger.hdf5\n",
            "Epoch 11/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 2.4558\n",
            "\n",
            "Epoch 00011: loss improved from 2.47210 to 2.45585, saving model to weights-improvement-11-2.4558-bigger.hdf5\n",
            "Epoch 12/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 2.4174\n",
            "\n",
            "Epoch 00012: loss improved from 2.45585 to 2.41740, saving model to weights-improvement-12-2.4174-bigger.hdf5\n",
            "Epoch 13/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.3896\n",
            "\n",
            "Epoch 00013: loss improved from 2.41740 to 2.38960, saving model to weights-improvement-13-2.3896-bigger.hdf5\n",
            "Epoch 14/50\n",
            "31102/31102 [==============================] - 153s 5ms/step - loss: 2.3659\n",
            "\n",
            "Epoch 00014: loss improved from 2.38960 to 2.36588, saving model to weights-improvement-14-2.3659-bigger.hdf5\n",
            "Epoch 15/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.3271\n",
            "\n",
            "Epoch 00015: loss improved from 2.36588 to 2.32714, saving model to weights-improvement-15-2.3271-bigger.hdf5\n",
            "Epoch 16/50\n",
            "31102/31102 [==============================] - 146s 5ms/step - loss: 2.2911\n",
            "\n",
            "Epoch 00016: loss improved from 2.32714 to 2.29113, saving model to weights-improvement-16-2.2911-bigger.hdf5\n",
            "Epoch 17/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 2.2454\n",
            "\n",
            "Epoch 00017: loss improved from 2.29113 to 2.24538, saving model to weights-improvement-17-2.2454-bigger.hdf5\n",
            "Epoch 18/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 2.2074\n",
            "\n",
            "Epoch 00018: loss improved from 2.24538 to 2.20738, saving model to weights-improvement-18-2.2074-bigger.hdf5\n",
            "Epoch 19/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 2.1540\n",
            "\n",
            "Epoch 00019: loss improved from 2.20738 to 2.15400, saving model to weights-improvement-19-2.1540-bigger.hdf5\n",
            "Epoch 20/50\n",
            "31102/31102 [==============================] - 147s 5ms/step - loss: 2.1023\n",
            "\n",
            "Epoch 00020: loss improved from 2.15400 to 2.10235, saving model to weights-improvement-20-2.1023-bigger.hdf5\n",
            "Epoch 21/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 2.0585\n",
            "\n",
            "Epoch 00021: loss improved from 2.10235 to 2.05852, saving model to weights-improvement-21-2.0585-bigger.hdf5\n",
            "Epoch 22/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 2.0024\n",
            "\n",
            "Epoch 00022: loss improved from 2.05852 to 2.00239, saving model to weights-improvement-22-2.0024-bigger.hdf5\n",
            "Epoch 23/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.9540\n",
            "\n",
            "Epoch 00023: loss improved from 2.00239 to 1.95401, saving model to weights-improvement-23-1.9540-bigger.hdf5\n",
            "Epoch 24/50\n",
            "31102/31102 [==============================] - 153s 5ms/step - loss: 1.9003\n",
            "\n",
            "Epoch 00024: loss improved from 1.95401 to 1.90029, saving model to weights-improvement-24-1.9003-bigger.hdf5\n",
            "Epoch 25/50\n",
            "31102/31102 [==============================] - 154s 5ms/step - loss: 1.8577\n",
            "\n",
            "Epoch 00025: loss improved from 1.90029 to 1.85774, saving model to weights-improvement-25-1.8577-bigger.hdf5\n",
            "Epoch 26/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.8009\n",
            "\n",
            "Epoch 00026: loss improved from 1.85774 to 1.80093, saving model to weights-improvement-26-1.8009-bigger.hdf5\n",
            "Epoch 27/50\n",
            "31102/31102 [==============================] - 151s 5ms/step - loss: 1.7550\n",
            "\n",
            "Epoch 00027: loss improved from 1.80093 to 1.75496, saving model to weights-improvement-27-1.7550-bigger.hdf5\n",
            "Epoch 28/50\n",
            "31102/31102 [==============================] - 151s 5ms/step - loss: 1.7144\n",
            "\n",
            "Epoch 00028: loss improved from 1.75496 to 1.71436, saving model to weights-improvement-28-1.7144-bigger.hdf5\n",
            "Epoch 29/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.6634\n",
            "\n",
            "Epoch 00029: loss improved from 1.71436 to 1.66342, saving model to weights-improvement-29-1.6634-bigger.hdf5\n",
            "Epoch 30/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.6144\n",
            "\n",
            "Epoch 00030: loss improved from 1.66342 to 1.61439, saving model to weights-improvement-30-1.6144-bigger.hdf5\n",
            "Epoch 31/50\n",
            "31102/31102 [==============================] - 153s 5ms/step - loss: 1.5730\n",
            "\n",
            "Epoch 00031: loss improved from 1.61439 to 1.57301, saving model to weights-improvement-31-1.5730-bigger.hdf5\n",
            "Epoch 32/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.5367\n",
            "\n",
            "Epoch 00032: loss improved from 1.57301 to 1.53673, saving model to weights-improvement-32-1.5367-bigger.hdf5\n",
            "Epoch 33/50\n",
            "31102/31102 [==============================] - 151s 5ms/step - loss: 1.4947\n",
            "\n",
            "Epoch 00033: loss improved from 1.53673 to 1.49471, saving model to weights-improvement-33-1.4947-bigger.hdf5\n",
            "Epoch 34/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.4517\n",
            "\n",
            "Epoch 00034: loss improved from 1.49471 to 1.45174, saving model to weights-improvement-34-1.4517-bigger.hdf5\n",
            "Epoch 35/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 1.4143\n",
            "\n",
            "Epoch 00035: loss improved from 1.45174 to 1.41434, saving model to weights-improvement-35-1.4143-bigger.hdf5\n",
            "Epoch 36/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 1.3753\n",
            "\n",
            "Epoch 00036: loss improved from 1.41434 to 1.37530, saving model to weights-improvement-36-1.3753-bigger.hdf5\n",
            "Epoch 37/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 1.3483\n",
            "\n",
            "Epoch 00037: loss improved from 1.37530 to 1.34831, saving model to weights-improvement-37-1.3483-bigger.hdf5\n",
            "Epoch 38/50\n",
            "31102/31102 [==============================] - 145s 5ms/step - loss: 1.3085\n",
            "\n",
            "Epoch 00038: loss improved from 1.34831 to 1.30846, saving model to weights-improvement-38-1.3085-bigger.hdf5\n",
            "Epoch 39/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.2719\n",
            "\n",
            "Epoch 00039: loss improved from 1.30846 to 1.27190, saving model to weights-improvement-39-1.2719-bigger.hdf5\n",
            "Epoch 40/50\n",
            "31102/31102 [==============================] - 147s 5ms/step - loss: 1.2485\n",
            "\n",
            "Epoch 00040: loss improved from 1.27190 to 1.24852, saving model to weights-improvement-40-1.2485-bigger.hdf5\n",
            "Epoch 41/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.2105\n",
            "\n",
            "Epoch 00041: loss improved from 1.24852 to 1.21055, saving model to weights-improvement-41-1.2105-bigger.hdf5\n",
            "Epoch 42/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.1872\n",
            "\n",
            "Epoch 00042: loss improved from 1.21055 to 1.18722, saving model to weights-improvement-42-1.1872-bigger.hdf5\n",
            "Epoch 43/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.1534\n",
            "\n",
            "Epoch 00043: loss improved from 1.18722 to 1.15344, saving model to weights-improvement-43-1.1534-bigger.hdf5\n",
            "Epoch 44/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.1373\n",
            "\n",
            "Epoch 00044: loss improved from 1.15344 to 1.13727, saving model to weights-improvement-44-1.1373-bigger.hdf5\n",
            "Epoch 45/50\n",
            "31102/31102 [==============================] - 148s 5ms/step - loss: 1.1004\n",
            "\n",
            "Epoch 00045: loss improved from 1.13727 to 1.10037, saving model to weights-improvement-45-1.1004-bigger.hdf5\n",
            "Epoch 46/50\n",
            "31102/31102 [==============================] - 149s 5ms/step - loss: 1.0685\n",
            "\n",
            "Epoch 00046: loss improved from 1.10037 to 1.06852, saving model to weights-improvement-46-1.0685-bigger.hdf5\n",
            "Epoch 47/50\n",
            "31102/31102 [==============================] - 145s 5ms/step - loss: 1.0510\n",
            "\n",
            "Epoch 00047: loss improved from 1.06852 to 1.05103, saving model to weights-improvement-47-1.0510-bigger.hdf5\n",
            "Epoch 48/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.0335\n",
            "\n",
            "Epoch 00048: loss improved from 1.05103 to 1.03352, saving model to weights-improvement-48-1.0335-bigger.hdf5\n",
            "Epoch 49/50\n",
            "31102/31102 [==============================] - 150s 5ms/step - loss: 1.0079\n",
            "\n",
            "Epoch 00049: loss improved from 1.03352 to 1.00786, saving model to weights-improvement-49-1.0079-bigger.hdf5\n",
            "Epoch 50/50\n",
            "31102/31102 [==============================] - 152s 5ms/step - loss: 0.9810\n",
            "\n",
            "Epoch 00050: loss improved from 1.00786 to 0.98096, saving model to weights-improvement-50-0.9810-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f20c4a404a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNx6o6E09tm7",
        "colab_type": "code",
        "outputId": "ab9ac33d-94cd-41ed-9315-1455e6d17cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\t#sys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" cements dry\n",
            "quickly, it is better to use a glue that water color will adhere to.\n",
            "regular hide glue o \"\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9iYgQigCRXm",
        "colab_type": "code",
        "outputId": "00052cd5-17fd-4e06-e82b-10ad6fee008b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# One lauer simplleRNN  to Generate Text \"KACHINA DOLLS\"\n",
        "import numpy\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"Work_text(1).txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the SimpleRNN model\n",
        "model = Sequential()\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(SimpleRNN(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, nb_epoch=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  31202\n",
            "Total Vocab:  67\n",
            "Total Patterns:  31102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 3.0884\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.08835, saving model to weights-improvement-01-3.0884-bigger.hdf5\n",
            "Epoch 2/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 3.0974\n",
            "\n",
            "Epoch 00002: loss did not improve from 3.08835\n",
            "Epoch 3/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 3.0273\n",
            "\n",
            "Epoch 00003: loss improved from 3.08835 to 3.02734, saving model to weights-improvement-03-3.0273-bigger.hdf5\n",
            "Epoch 4/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.9698\n",
            "\n",
            "Epoch 00004: loss improved from 3.02734 to 2.96977, saving model to weights-improvement-04-2.9698-bigger.hdf5\n",
            "Epoch 5/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.9368\n",
            "\n",
            "Epoch 00005: loss improved from 2.96977 to 2.93685, saving model to weights-improvement-05-2.9368-bigger.hdf5\n",
            "Epoch 6/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.9017\n",
            "\n",
            "Epoch 00006: loss improved from 2.93685 to 2.90169, saving model to weights-improvement-06-2.9017-bigger.hdf5\n",
            "Epoch 7/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.8790\n",
            "\n",
            "Epoch 00007: loss improved from 2.90169 to 2.87899, saving model to weights-improvement-07-2.8790-bigger.hdf5\n",
            "Epoch 8/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.8580\n",
            "\n",
            "Epoch 00008: loss improved from 2.87899 to 2.85799, saving model to weights-improvement-08-2.8580-bigger.hdf5\n",
            "Epoch 9/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.8436\n",
            "\n",
            "Epoch 00009: loss improved from 2.85799 to 2.84356, saving model to weights-improvement-09-2.8436-bigger.hdf5\n",
            "Epoch 10/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.8247\n",
            "\n",
            "Epoch 00010: loss improved from 2.84356 to 2.82466, saving model to weights-improvement-10-2.8247-bigger.hdf5\n",
            "Epoch 11/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.8009\n",
            "\n",
            "Epoch 00011: loss improved from 2.82466 to 2.80087, saving model to weights-improvement-11-2.8009-bigger.hdf5\n",
            "Epoch 12/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.7802\n",
            "\n",
            "Epoch 00012: loss improved from 2.80087 to 2.78019, saving model to weights-improvement-12-2.7802-bigger.hdf5\n",
            "Epoch 13/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.7592\n",
            "\n",
            "Epoch 00013: loss improved from 2.78019 to 2.75916, saving model to weights-improvement-13-2.7592-bigger.hdf5\n",
            "Epoch 14/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.7515\n",
            "\n",
            "Epoch 00014: loss improved from 2.75916 to 2.75150, saving model to weights-improvement-14-2.7515-bigger.hdf5\n",
            "Epoch 15/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.7314\n",
            "\n",
            "Epoch 00015: loss improved from 2.75150 to 2.73140, saving model to weights-improvement-15-2.7314-bigger.hdf5\n",
            "Epoch 16/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.7130\n",
            "\n",
            "Epoch 00016: loss improved from 2.73140 to 2.71305, saving model to weights-improvement-16-2.7130-bigger.hdf5\n",
            "Epoch 17/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.6903\n",
            "\n",
            "Epoch 00017: loss improved from 2.71305 to 2.69028, saving model to weights-improvement-17-2.6903-bigger.hdf5\n",
            "Epoch 18/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.6834\n",
            "\n",
            "Epoch 00018: loss improved from 2.69028 to 2.68344, saving model to weights-improvement-18-2.6834-bigger.hdf5\n",
            "Epoch 19/50\n",
            "31102/31102 [==============================] - 56s 2ms/step - loss: 2.6686\n",
            "\n",
            "Epoch 00019: loss improved from 2.68344 to 2.66857, saving model to weights-improvement-19-2.6686-bigger.hdf5\n",
            "Epoch 20/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.6544\n",
            "\n",
            "Epoch 00020: loss improved from 2.66857 to 2.65438, saving model to weights-improvement-20-2.6544-bigger.hdf5\n",
            "Epoch 21/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.6376\n",
            "\n",
            "Epoch 00021: loss improved from 2.65438 to 2.63761, saving model to weights-improvement-21-2.6376-bigger.hdf5\n",
            "Epoch 22/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.6272\n",
            "\n",
            "Epoch 00022: loss improved from 2.63761 to 2.62715, saving model to weights-improvement-22-2.6272-bigger.hdf5\n",
            "Epoch 23/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.6104\n",
            "\n",
            "Epoch 00023: loss improved from 2.62715 to 2.61043, saving model to weights-improvement-23-2.6104-bigger.hdf5\n",
            "Epoch 24/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.5991\n",
            "\n",
            "Epoch 00024: loss improved from 2.61043 to 2.59907, saving model to weights-improvement-24-2.5991-bigger.hdf5\n",
            "Epoch 25/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.5906\n",
            "\n",
            "Epoch 00025: loss improved from 2.59907 to 2.59056, saving model to weights-improvement-25-2.5906-bigger.hdf5\n",
            "Epoch 26/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.5788\n",
            "\n",
            "Epoch 00026: loss improved from 2.59056 to 2.57882, saving model to weights-improvement-26-2.5788-bigger.hdf5\n",
            "Epoch 27/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.5636\n",
            "\n",
            "Epoch 00027: loss improved from 2.57882 to 2.56362, saving model to weights-improvement-27-2.5636-bigger.hdf5\n",
            "Epoch 28/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.5542\n",
            "\n",
            "Epoch 00028: loss improved from 2.56362 to 2.55416, saving model to weights-improvement-28-2.5542-bigger.hdf5\n",
            "Epoch 29/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.5381\n",
            "\n",
            "Epoch 00029: loss improved from 2.55416 to 2.53812, saving model to weights-improvement-29-2.5381-bigger.hdf5\n",
            "Epoch 30/50\n",
            "31102/31102 [==============================] - 59s 2ms/step - loss: 2.5255\n",
            "\n",
            "Epoch 00030: loss improved from 2.53812 to 2.52547, saving model to weights-improvement-30-2.5255-bigger.hdf5\n",
            "Epoch 31/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.5162\n",
            "\n",
            "Epoch 00031: loss improved from 2.52547 to 2.51617, saving model to weights-improvement-31-2.5162-bigger.hdf5\n",
            "Epoch 32/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.5039\n",
            "\n",
            "Epoch 00032: loss improved from 2.51617 to 2.50387, saving model to weights-improvement-32-2.5039-bigger.hdf5\n",
            "Epoch 33/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.5020\n",
            "\n",
            "Epoch 00033: loss improved from 2.50387 to 2.50199, saving model to weights-improvement-33-2.5020-bigger.hdf5\n",
            "Epoch 34/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4812\n",
            "\n",
            "Epoch 00034: loss improved from 2.50199 to 2.48123, saving model to weights-improvement-34-2.4812-bigger.hdf5\n",
            "Epoch 35/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4704\n",
            "\n",
            "Epoch 00035: loss improved from 2.48123 to 2.47044, saving model to weights-improvement-35-2.4704-bigger.hdf5\n",
            "Epoch 36/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4664\n",
            "\n",
            "Epoch 00036: loss improved from 2.47044 to 2.46636, saving model to weights-improvement-36-2.4664-bigger.hdf5\n",
            "Epoch 37/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4529\n",
            "\n",
            "Epoch 00037: loss improved from 2.46636 to 2.45291, saving model to weights-improvement-37-2.4529-bigger.hdf5\n",
            "Epoch 38/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4469\n",
            "\n",
            "Epoch 00038: loss improved from 2.45291 to 2.44689, saving model to weights-improvement-38-2.4469-bigger.hdf5\n",
            "Epoch 39/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4313\n",
            "\n",
            "Epoch 00039: loss improved from 2.44689 to 2.43133, saving model to weights-improvement-39-2.4313-bigger.hdf5\n",
            "Epoch 40/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4307\n",
            "\n",
            "Epoch 00040: loss improved from 2.43133 to 2.43072, saving model to weights-improvement-40-2.4307-bigger.hdf5\n",
            "Epoch 41/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4199\n",
            "\n",
            "Epoch 00041: loss improved from 2.43072 to 2.41992, saving model to weights-improvement-41-2.4199-bigger.hdf5\n",
            "Epoch 42/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4119\n",
            "\n",
            "Epoch 00042: loss improved from 2.41992 to 2.41188, saving model to weights-improvement-42-2.4119-bigger.hdf5\n",
            "Epoch 43/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4092\n",
            "\n",
            "Epoch 00043: loss improved from 2.41188 to 2.40917, saving model to weights-improvement-43-2.4092-bigger.hdf5\n",
            "Epoch 44/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4018\n",
            "\n",
            "Epoch 00044: loss improved from 2.40917 to 2.40182, saving model to weights-improvement-44-2.4018-bigger.hdf5\n",
            "Epoch 45/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4032\n",
            "\n",
            "Epoch 00045: loss did not improve from 2.40182\n",
            "Epoch 46/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.4069\n",
            "\n",
            "Epoch 00046: loss did not improve from 2.40182\n",
            "Epoch 47/50\n",
            "31102/31102 [==============================] - 57s 2ms/step - loss: 2.3891\n",
            "\n",
            "Epoch 00047: loss improved from 2.40182 to 2.38914, saving model to weights-improvement-47-2.3891-bigger.hdf5\n",
            "Epoch 48/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.4059\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.38914\n",
            "Epoch 49/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.3821\n",
            "\n",
            "Epoch 00049: loss improved from 2.38914 to 2.38208, saving model to weights-improvement-49-2.3821-bigger.hdf5\n",
            "Epoch 50/50\n",
            "31102/31102 [==============================] - 58s 2ms/step - loss: 2.3709\n",
            "\n",
            "Epoch 00050: loss improved from 2.38208 to 2.37094, saving model to weights-improvement-50-2.3709-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb016bf7cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBx9MvE1aXbw",
        "colab_type": "code",
        "outputId": "426a6d44-96c8-400e-82ec-064b178ec575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\t#sys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" e along these lines. you\n",
            "will probably get a lot of ideas when traveling, but here are a couple\n",
            "that \"\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}